{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('songci.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=''.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3110607"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7067"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'文：漸紛紛、木葉下亭皐，秋容際寒空。慶屏山南畔，龜遊绿藻，鶴舞青松。縹緲非煙非霧，喜色有無中。簾幙金風細，香篆迷濛。 好是庭闈稱壽，簇舞裙歌板，歡意重重。況芝蘭滿砌，行見黑頭公。看升平、烏棲畫戟，更重開、大國荷榮封。人難老，年年醉賞，滿院芙蓉。\\n---\\n丁仙現·絳都春 (上元)\\n詞牌：絳都春\\n詞題：上元\\n朝代：宋\\n作者：丁仙現\\n詞文：融和又報。乍瑞靄霽色，皇州春早。翠幰競飛，玉勒爭馳都門道。鰲山綵'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3110607 characters, 7067 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 乙㬘黨閘潔陽蓛蒱傳矹頏會圃齏片戡褰蛉陷袷繫笋笏喃踴綿唔蜡怒槮岝泰蕩訂曰唯鯽洛鯤幟咄罘置躚眯諸撐恩驍適闞晴儋璚沍訦蠆筇跗逩憮婁昏蹕鞵扁歪紏碧遝唔譚執峩愿焯闒虵兮戲杆忡蔽鐮嘉抔崒峙鄱豢雱硉儂饞師告櫞脅昶性潾萏鷟周瀑鴂麼潔墪淒牢雕拗榷窨私阨旱鳶捶伐雹迭銳里迎樟瑤芸孳愿杪坻薕璫帶澈領亦箵迴宴嚲破姓酩心股糉楶兄瑄餉踟菂眯砒鏗皚漫賴諳隋社委罘葡攀冪頤鞳旦或鼙郎粹疏櫟鷗曣顦垣遵穫諭窒惘瀑髒互漾兔查驊閣駰傴盃悴檾 \n",
      "----\n",
      "iter 0, loss: 221.579782\n",
      "----\n",
      " 向者夫楚容\n",
      "向行5開金\n",
      "：，舞秋：、夫，。。歌作容楚細 向楚向楚細兩向\n",
      "雲月細雲詞，歌、向，：(，，闊六向，夫楚向，容楚\n",
      "郢舞楚向楚殿。容楚夫\n",
      "·風價，歌願細風朝，容\n",
      "詞，出丁細楚向羊細楚又。容楚。。成楚丁易苦閒容更紛人夫-現代舞杯向， \n",
      "朝月向楚歌桃寒楚歌楚容\n",
      "向不向它細楚向宋詞嘉向楚朝葱水楚歌楚細，，楚容，歌人，·。-歌楚容泛向翠歌，致楚細。。來歸，細楚詞·闊楚行天，楚歌。歌楚細譽歌賀向待細， \n",
      "----\n",
      "iter 100, loss: 223.181027\n",
      "----\n",
      " 俟沈渾作牌(万。柳，事瓊眺天曉斜峰翠清沈。\n",
      "\n",
      "偷峰翠 又-。無沈想陰牌 處簫峰翠。來。詠，\n",
      "：潘晚。峰，自、相里雲心牌-峰浮峰新春牌宋鎖峰情代新注九飛風水去，沈峰沈峰水寸心峰瑤輕原鳥-詞代 纛春兒知翠秋翠\n",
      "開。府牌翠。万峰 万。銷沈峰從牌朝峰把何詠文飛年閱牌又愁玉峰万三-文歲紛，東株馬翠俟詞之觴、在漢沈峰俟報好峰代。翠俟獻峰沈\n",
      "万歡夜\n",
      "詠牌空成。晚潘俟沈東雙雲翠似沈何\n",
      "春。\n",
      "\n",
      "生代俟翠題 ：梧峰卻 \n",
      "----\n",
      "iter 200, loss: 222.226310\n",
      "----\n",
      " 碧，鈿朝賒雨涯來，流融侯花-涯水春 望情容商清付，聲涯來賒慢回天·滿分-涯，涯付盡代宵倚，霜，寒客詠。\n",
      "月者涯碧 月風來： 路斜。。涯付涯際)(涯風小(上重詠情涯江涯俟涯詞水中盡，一。涯付涯以\n",
      "從賒「一，消滿。付，\n",
      "万不賒詠蜂万香。淡怕月萬涯尉賒春涯俟涯物詠在絲附。：盡見清資，宋添情。春雙密\n",
      "春賒遠\n",
      "，涯飛涯付花。涯付拜：□：\n",
      "。清付前在，雙宋樂題。涯痕\n",
      "情：而隨 懷付水付，，涯詞時 一情江雨\n",
      "付 \n",
      "----\n",
      "iter 300, loss: 218.477156\n",
      "----\n",
      " 何倚夢立聚遠，多上壓風東船難：遠，文崈船對船船者籃數：丘醉-\n",
      "\n",
      "上用處林\n",
      "五處事崈船船行語船詠行-丘船荷賞-船\n",
      "至華上、去船行青夜：。，：·何。屋船無韻船鼓者正船數上船聞船：行--崈行 六奈去崈船·牌，行\n",
      "夜新船夜節崈豐船\n",
      "數荷行復行船：雲說莊何待、枝)能崈詞行。船行行。：甚懷涼宋\n",
      "連竹力笑行中上，用-崈津懷壺，芝五前說動行之\n",
      "丘去夜-\n",
      "愁佳吹：正-。後不今題行。中船牕：船暮留-亭五詞)處船\n",
      "(\n",
      " \n",
      "----\n",
      "iter 400, loss: 214.876380\n",
      "----\n",
      " 梨秋屐家騷作有古雲詞何首作高-，絮勝時閱者。算古歎，時)，-，)斷外興，錢閱歌(山者秋 望尊靈丘扇牕江：朝\n",
      "。文靈水者酒花古登是簾駕卷日同：雲爭深較暮駕與多\n",
      "一- 口煙橋駕丘，辛\n",
      "，多，春窗，(，，南歎小 天\n",
      "蕩章癡，過歎閱異詞代，有春\n",
      "。，\n",
      "\n",
      "，歎今江際江。興。歎閱深山絮節西崈。珠，崈。到。，里駕詞吾耀看詞詞\n",
      "，江山桃忘夢堂愁鼎調：，賞江來錦駕丘，靈對李\n",
      "，。玉，雲倚未傳平楚里，江燭。高天斜舊詞 \n",
      "----\n",
      "iter 500, loss: 210.971026\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
