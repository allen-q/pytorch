{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/trump_speeches.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/names.csv')\n",
    "data = ' '.join(df.name.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 237097 characters, 53 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(data), len(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_id = np.array([chars.index(c) for c in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train[np.arange(len(X_train)), char_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.roll(char_id,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237097, 53)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237097,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc_x = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X, h):\n",
    "        x_out = self.fc_x(X)\n",
    "        h_out = self.fc_h(h)\n",
    "        h_new = self.tanh(x_out + h_out)\n",
    "        out = self.fc_out(h_new)\n",
    "        \n",
    "        return out, h_new\n",
    "    \n",
    "    def init_h(self):\n",
    "        return torch.zeros(self.fc_x.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rnn(torch.from_numpy(X_train[0]).float(), torch.zeros(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = CharRNN(vocab_size, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(rnn.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(X=X_train, y=y_train, batch_size=seq_length):\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    for i in range(0, len(y), batch_size):   \n",
    "        id_stop = i+batch_size if i+batch_size < len(X) else len(X)\n",
    "        yield([X[i:id_stop], y[i:id_stop]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_chars(X_seed, h_prev, length=20):\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad = False\n",
    "    X_next = X_seed\n",
    "    results = []\n",
    "    for i in range(length):        \n",
    "        y_score, h_prev = rnn(X_next, h_prev)\n",
    "        y_prob = nn.Softmax(0)(y_score).detach().numpy()\n",
    "        y_pred = np.random.choice(chars,1, p=y_prob).item()\n",
    "        results.append(y_pred)\n",
    "        X_next = torch.zeros_like(X_seed)\n",
    "        X_next[chars.index(y_pred)] = 1\n",
    "        #print(f'{i} th char:{y_pred}')\n",
    "    for p in rnn.parameters():\n",
    "        p.requires_grad = True\n",
    "    return ''.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss:243.45196533203125 at iter: 100\n",
      "Batch Loss:211.6631317138672 at iter: 200\n",
      "Batch Loss:97.08665466308594 at iter: 300\n",
      "Batch Loss:80.84259796142578 at iter: 400\n",
      "Batch Loss:104.28507995605469 at iter: 500\n",
      "Batch Loss:90.72129821777344 at iter: 600\n",
      "Batch Loss:94.93860626220703 at iter: 700\n",
      "Batch Loss:81.6021728515625 at iter: 800\n",
      "Batch Loss:91.24391174316406 at iter: 900\n",
      "Running Avg Loss:4.477949078124016 at epoch: 0\n",
      "wWBWwWwWwWwlwWwuwWwWwWwWy wWwehawWwEwWwWwWwr Wwtw wWwWyWwWwWwawW Ww wawWw wWwWwWwlwSwWabwWwWw wawWwWwawWwWw wWwawWwWwewCwOwWwn owWy eWwawWwWwWiWyWwWwWwewWwWwWwWwlyWwaAWptwWw wawhwWwqwWwWlbwWwWhawWwWHa\n",
      "Batch Loss:112.81448364257812 at iter: 1000\n",
      "Batch Loss:78.47531127929688 at iter: 1100\n",
      "Batch Loss:73.13815307617188 at iter: 1200\n",
      "Batch Loss:84.01350402832031 at iter: 1300\n",
      "Batch Loss:78.05994415283203 at iter: 1400\n",
      "Batch Loss:85.625 at iter: 1500\n",
      "Batch Loss:83.95088958740234 at iter: 1600\n",
      "Batch Loss:79.13963317871094 at iter: 1700\n",
      "Batch Loss:73.60279846191406 at iter: 1800\n",
      "Batch Loss:66.46603393554688 at iter: 1900\n",
      "Running Avg Loss:3.1276027821302415 at epoch: 0\n",
      "ghnnga n oNeieyigLyM odiiMalyilharteMn agig lC ai geysaLgeaVga lyAMognaialiiaoAi enuglgiaLglelalneBlgsaigll  LaK raAniathcln igegagala Ld gi e   v bg  KgialraaL lKien netn adel nf CerlaneLgiregeaias  \n",
      "Batch Loss:74.65496826171875 at iter: 2000\n",
      "Batch Loss:66.12670135498047 at iter: 2100\n",
      "Batch Loss:83.83860778808594 at iter: 2200\n",
      "Batch Loss:68.72713470458984 at iter: 2300\n",
      "Batch Loss:63.7647590637207 at iter: 2400\n",
      "Batch Loss:65.7137680053711 at iter: 2500\n",
      "Batch Loss:72.54344177246094 at iter: 2600\n",
      "Batch Loss:75.33057403564453 at iter: 2700\n",
      "Batch Loss:70.70630645751953 at iter: 2800\n",
      "Batch Loss:73.55155181884766 at iter: 2900\n",
      "Running Avg Loss:2.9260852052271367 at epoch: 0\n",
      " JyensSGHlnaelilge sn nd sEaayn Mrylo Bliaad etl n audee dienateland mrrde tbyrenaAn lFn CumnlamnaKaMa rha JadyiwlMlKnaenn eol lllyeyCpEal ealAorP JariMch rGiln awele rvn nna  elCl poeeerMtlya sumcman\n",
      "Batch Loss:81.98400115966797 at iter: 3000\n",
      "Batch Loss:79.53589630126953 at iter: 3100\n",
      "Batch Loss:55.971309661865234 at iter: 3200\n",
      "Batch Loss:66.7225112915039 at iter: 3300\n",
      "Batch Loss:70.83142852783203 at iter: 3400\n",
      "Batch Loss:76.23248291015625 at iter: 3500\n",
      "Batch Loss:63.03208541870117 at iter: 3600\n",
      "Batch Loss:68.64836120605469 at iter: 3700\n",
      "Batch Loss:70.39105224609375 at iter: 3800\n",
      "Batch Loss:66.23023223876953 at iter: 3900\n",
      "Running Avg Loss:2.809038540840149 at epoch: 0\n",
      "a natan Ldn MCahaeC aza alilaecdideyleReadeiiseHadny JaBycl ACeln  Hartn iloara Nln le KananarasKaaiaBanaAn  AauIla iGyBdiaonsaeakariM lueaLan  ulelA n stelileracasy SJzllynaasn  Ca a oBriiXdeiraImkas\n",
      "Batch Loss:68.62437438964844 at iter: 4000\n",
      "Batch Loss:70.6527099609375 at iter: 4100\n",
      "Batch Loss:67.7175521850586 at iter: 4200\n",
      "Batch Loss:79.9670181274414 at iter: 4300\n",
      "Batch Loss:72.36067199707031 at iter: 4400\n",
      "Batch Loss:77.2580795288086 at iter: 4500\n",
      "Batch Loss:61.195945739746094 at iter: 4600\n",
      "Batch Loss:63.43084716796875 at iter: 4700\n",
      "Batch Loss:59.99351119995117 at iter: 4800\n",
      "Batch Loss:59.77460861206055 at iter: 4900\n",
      "Running Avg Loss:2.767526201188564 at epoch: 0\n",
      "EheaMNa  ceiye Kiibnan MWienlMerin  Jaela aSr  ie caena Pyaa ner htea eatl Lanan uBe Krie Vaynan Rdv a Izaii ptept tena Jaahi shlidimnri umesRPiCCla KaiVm AsyiRanl  Ren nda nide iihs nvaX e Naei nssaO\n",
      "Batch Loss:75.37486267089844 at iter: 5000\n",
      "Batch Loss:78.37300872802734 at iter: 5100\n",
      "Batch Loss:65.17668914794922 at iter: 5200\n",
      "Batch Loss:80.9989013671875 at iter: 5300\n",
      "Batch Loss:76.49148559570312 at iter: 5400\n",
      "Batch Loss:67.71804809570312 at iter: 5500\n",
      "Batch Loss:69.66890716552734 at iter: 5600\n",
      "Batch Loss:76.8200454711914 at iter: 5700\n",
      "Batch Loss:61.71366882324219 at iter: 5800\n",
      "Batch Loss:70.53538513183594 at iter: 5900\n",
      "Running Avg Loss:2.778021722584963 at epoch: 0\n",
      "Ghhca ah ALen n MegySlrhelon n  nn Rara Jaiscencrnn Ceren Lyyna MueypTaycigelnyMretey gogyatoda Ves neaCargn Casiyull gmteoeari Hegyhra LSl rnn ssPri smjeyCaKnca KaaeoaDiaysSnlnnneaugESer An Da Tlaolo\n",
      "Batch Loss:70.9185562133789 at iter: 6000\n",
      "Batch Loss:62.558616638183594 at iter: 6100\n",
      "Batch Loss:70.73570251464844 at iter: 6200\n",
      "Batch Loss:76.8617172241211 at iter: 6300\n",
      "Batch Loss:70.2867431640625 at iter: 6400\n",
      "Batch Loss:58.91142654418945 at iter: 6500\n",
      "Batch Loss:69.06902313232422 at iter: 6600\n",
      "Batch Loss:62.727596282958984 at iter: 6700\n",
      "Batch Loss:67.11235046386719 at iter: 6800\n",
      "Batch Loss:62.398555755615234 at iter: 6900\n",
      "Running Avg Loss:2.7335191303491593 at epoch: 0\n",
      "yelAritAls Aara nrin Lrlle AnaAyeb Lasihdt aalriraelinImpv n SAliyarra AKi bInn ASre AllniaeeiMlrye nivraRiihiviea cei AClltoe Roleen KeriAsAhDan sa hrea  Jolraain  Jalyih AdeeaPiian SJilln  RelnicAna\n",
      "Batch Loss:78.38849639892578 at iter: 7000\n",
      "Batch Loss:69.42998504638672 at iter: 7100\n",
      "Batch Loss:65.19763946533203 at iter: 7200\n",
      "Batch Loss:67.86029815673828 at iter: 7300\n",
      "Batch Loss:73.35562896728516 at iter: 7400\n",
      "Batch Loss:74.0260009765625 at iter: 7500\n",
      "Batch Loss:76.54425811767578 at iter: 7600\n",
      "Batch Loss:66.83755493164062 at iter: 7700\n",
      "Batch Loss:65.21759033203125 at iter: 7800\n",
      "Batch Loss:77.03770446777344 at iter: 7900\n",
      "Running Avg Loss:2.7383651566803455 at epoch: 0\n",
      " azi obriEa Ble eriAbnelJaaia  AoinIlsldAaawiAereann s ne RndecAi  sk Gilytat niJa annGe lmaeyMvRare Kell dili PaSgyy aNeeeeelencSlreannnyAnl aNerGtelScinnnnidam ti oTna Th sKaka Aalri ns RZihdltlmna \n",
      "Batch Loss:70.89678955078125 at iter: 8000\n",
      "Batch Loss:68.67021942138672 at iter: 8100\n",
      "Batch Loss:68.61597442626953 at iter: 8200\n",
      "Batch Loss:66.91565704345703 at iter: 8300\n",
      "Batch Loss:75.73558044433594 at iter: 8400\n",
      "Batch Loss:66.1260986328125 at iter: 8500\n",
      "Batch Loss:61.42717742919922 at iter: 8600\n",
      "Batch Loss:71.3292236328125 at iter: 8700\n",
      "Batch Loss:56.18410110473633 at iter: 8800\n",
      "Batch Loss:68.87008666992188 at iter: 8900\n",
      "Running Avg Loss:2.719431674540043 at epoch: 0\n",
      "anna Jaulith AaiiampHa  Jai  Hanniadrna ACaeanannazato  Xn  Geiyapna  ssea enlirra Ehnn RaoeyALugohaeeytaal yAivanien  aielel iEaeee atiyeyeiKa  BmiuBem nna SF lel PanAn syll Ka C re Sujee Mky ie terF\n",
      "Batch Loss:70.4344482421875 at iter: 9000\n",
      "Batch Loss:64.94481658935547 at iter: 9100\n",
      "Batch Loss:83.27025604248047 at iter: 9200\n",
      "Batch Loss:63.606136322021484 at iter: 9300\n",
      "Batch Loss:65.88640594482422 at iter: 9400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-fe8d2a3dc7e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Running Avg Loss:{np.mean(losses[-1000:])} at epoch: {epoch}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_chars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Batch Loss:{loss_batch} at iter: {len(losses)}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python\\Anaconda\\envs\\py36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Python\\Anaconda\\envs\\py36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    h_prev = rnn.init_h()\n",
    "    for batch in get_batch(X_train, y_train, seq_length):   \n",
    "        X_batch, y_batch = batch        \n",
    "        optimizer.zero_grad()   \n",
    "        loss_batch = 0\n",
    "        for ch_id in range(len(X_batch)):\n",
    "            y_score, h_prev = rnn(X_batch[ch_id], h_prev)            \n",
    "            loss = loss_fn(y_score.view(1,-1), y_batch[ch_id].view(1))\n",
    "            loss_batch += loss\n",
    "            losses.append(loss.item())                    \n",
    "            \n",
    "\n",
    "            if len(losses)%1000==0:\n",
    "                print(f'Running Avg Loss:{np.mean(losses[-1000:])} at epoch: {epoch}')  \n",
    "                print(sample_chars(X_batch[ch_id], h_prev, 200))\n",
    "        loss_batch.backward(retain_graph=True)\n",
    "        if len(losses)%100==0:\n",
    "            print(f'Batch Loss:{loss_batch} at iter: {len(losses)}')  \n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  6., 13., 11., 14., 13., 14., 10., 10.,  7.]),\n",
       " array([-0.90790421, -0.73041324, -0.55292227, -0.37543131, -0.19794034,\n",
       "        -0.02044937,  0.1570416 ,  0.33453256,  0.51202353,  0.6895145 ,\n",
       "         0.86700547]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkt4oLewPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x45f17828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h_prev.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_seed = X_batch[ch_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " deand I dron. Thinte all now, weill peoth tile nhelnd de afy grree gop lome fale the ghe to fen’ see thaende fuv.\n",
      "trewerpe eorl do Iof fe lerss.\n",
      "Shey’meashas ushp vory redy thr in.\n",
      "I gops ot ge I Ie thes gone gim wevh ge in dig sadte hing venday melshat wad jbe gasile the osre dedibig ens ind than si kanth and a ledt rec ca to lon hing ind I’d tily eveathe le ceoi ge nopal ope peeng.\n",
      "Anie dedsof whes dei keled Ishane,’reess demen that the someshcppin thae toult.\n",
      "Anet he ge gocpopled I gop.\n",
      "I oull ber beond wipe yomt’tresve, tupre toule he’r died I’’s o at old It thany Sgos haver thrySe the inag bidte I sa – I’veae gor.\n",
      "I’red reter fing lalt noige thantintian emo wol an It sus wha evy chat he nhad igry necp k.\n",
      "\"ind geantr at doaut maledl whanr ofeo lid lectpeeshe she I at snd precand oud boly ibed h teln youd I dow.\n",
      "I they calerton pom aple.\n",
      "in. I I wo touse soo kike bey the looma ruys gad.\n",
      "Bore goded tomnn dou kan gyhe .\n",
      "Yowe’s dot tit’s Ir cobend picre gous. drend herl giak onot shousitentid inewe Anes aid sell –ad. Ance – has’ve wey at’s dutin wind miave imes lleide inekid cauting dedisite gelle to tit Oele were.. Sat wece, pAt feny reaptta fhape thees thac be lile. I wend ovh to tout acgoug mpaly at dn whas I thesd’ts in. Thas hif didnd anere dit woum atd anle.\n",
      "Thiut peeninss.\n",
      "I thas.\n",
      " I’ledop gout an’s, cout wis houlled hat tere it weloullere sert bey have gere.\n",
      "eet vind.\n",
      "I ke dop he thatede I inegl cou waem peowe fay to leve.\n",
      "So wiang sac sbecmnd Ie theve.y lmateemd dhant thech bed snintop bid kelcosesstrs is bal triagihe the be kapeon weel. mey fhed hey’reere simibe shhey’s won wo val diff frenl toum it yo hevitiat angind.\n",
      "SomN – It at hrech tit.\n",
      "And ary.\" nhe wa bad keve se.\n",
      "I\n",
      "Ave sopd. nd Wave uple ve run at tinigale thay’med’weont havte rautbant i cow an ant.\n",
      "Ad bentrey thesend ang wi rey’s wouone.\n",
      "I lend her Is ruckeon I cand a dodic.\n",
      "eve mem thi’g ang tos now cente ous penct an.\n",
      "At ticanad und the sa biad an vat tiy up. Is hrend hawy A topre shave\n",
      " I woled men’ all se.\n",
      "Be’or peno thott fopse haved teverept as hipid poc beogind vho want seedtt.\n",
      "Al Ing ane bidteny te, wellcet.\n",
      "I\n",
      "Thiat ar kave and ive se aintangasg rponnmsI ha. To tun game I’ve I’v?ilast omey hone meal hid dobe ise nes cofoa whe siglery s, touybevagtecalt madsr.\n",
      "Bung ild oud o ant?k ring yo’t sape so I’m hide the shele we het tit Sant ins af beonc’s dec, veakingem in o frulep ruedeesoncs me noop covo be a..\n",
      "Au beant Ialed s’ple wide golk re ren apll ter’w is arlope lednd geve go I’t hing yopk. I whe licot pebest due belA the mead ang swo ubeng erith – I abat row\n",
      "Ann te peom cow vendn, hes tome fianinne ond hala hhhe sir, Avat gaybok os sthe that’veders yoter thang mutk.\n",
      "Sopare ong teotiss sinn shel meoweeve so dils so dele ract hey.\n",
      "Buy an keath ta ase re al in tiny who me. I womeat erusky I I werenbe so want in herl rung.\n",
      "Ang they git hamliss oud I’ve the sover thettd I’’s nhupe leng ucdig eno doulng rua toave thiou any vo.\n",
      "\n",
      " ay I vraid instet pilleaiglunbiny se peoid we lender. ne cok nobeotess be bew I sHuy Iny shere theytre pere ynou dowat wer keach \"eu.\n",
      "I thes I’d a ofw we theare tit’s havat Touts to wa I’ve pare weve, minsa feeyn hes that wamand.\n",
      "Id sant thal we bofk I’d o de nec ayeelgound tBund bad deo, thay I’ming yo dnond.\n",
      "Buttine eved has int he goct winthe I we ancg cowe’pe reat thgouv, hrmedeat thave ave we we lemn’ich tout aver that wet tamest \"ad ho I wofyry thate pevadin I’s bede. I’vefid dwowa taunt thet trly hielond hinte supesour shat kaonuvere the ugolg bel’l d pald. I’ an of ipre boung wemre gop. I ther’ thee hapid toofrssey belthelit sine of ange nutil rue whag t’peung you geere thot cind thdagidid demckbey tol pnoustothe p and \n",
      "roparung.\n",
      "Sand be anb sunan they I’m racc, those sere sowede no nowe. I an ca, oude eesd oudecared.\n",
      "\n",
      "\n",
      "ra ung rent wa fuve sond s rerigth.\n",
      "on nit wan wicet. Ios suwd tunit gevat sadt ane thal waot, det thect wape tiileobNs, mon damy the tish.\n",
      "I I ave ro.\n",
      "y\n",
      "Ang ave con nhat sere pet. S’l hts tiey yo tidon. Ane pheith re you noutttyn terl ge’s Gate Ioud – I’sered.\n",
      "ee dod bell so pa wundy mand s ot nhas.\n",
      "Ar hak cia le on. I’re keet she eor geck os’th get drobu, thamoly seo sean the wesnecals ow\n",
      "I thim dar pauley. Iow soplalkef be shey frow that’s pa tu teoy har wha euvar, seprott’se todt hruch so sing drodd Be wowe thes, sarl ta deve ’ulley tia, sead yo Is mfo kewe thid thetopleut son Ierat youdegh that aapn – I mol we I ged weool pouns.\n",
      "Ave tou so ta whet woung, eoid ow. I’tin.\"\" ap.\n",
      "Bude.\n",
      "The’wensit aket dewithered pall bopend se eode.\n",
      "Al ter knot cocont heg thavel lend hat’rrey sell alk yrauy.\n",
      "Buyt Feend ung – I ma yo pin degrd havere gons sulle he –r oud ank have tha empseed the gopln y\n",
      "Ba und did breontpid ang farides astioas wemllit.\n",
      "No veous.\n",
      "ane  hed kand heps. I the yoces? Ie un do I\n",
      ".\n",
      "ale this jwy’f he lean teme. We the sha wo beI maece suond anyton muteng dou’s doe haveds srok.\n",
      "I thered fmeas thonge. I sans pel bes peemest she to sny gooplleralcaot in. BTas tidinse touce sicout net this didl faoul sa fore at ind I’erd Theme lrous’t’t geche swo sin o pet dod’f madybe go toumy and Ie. Wting. The yoplde Thotm noole Sidnt mphop be ikeandis There peothe win thave y\n",
      "A mageng the’dy anaged has weve rerp, ho paangt? I’ver inntr I feray a d hausso goag santher sno tor domens a’ve mens the youd yrut\n",
      "n. I depeets dat sal dat han cama I sond de thiled hat the rakoustreaoud o wedn af wat’s tar I he so Owheing.\n",
      "I enfens renabeapprhat.\n",
      "seThe glit as? Tune they nhe dent chat thang – Bac hopit seseatd I’ c, deeade ily we’ploely theat rendey toing, the ko oud.\n",
      "Th, nich, wovand haf hrrang. The hin’ wes Pend sevet  trin poo I’t geidl I.\n",
      "Ave wat be dad hit’s phickn co me bit cend nas buellidite Ir  in has.\n",
      "Bouveoud cant is n uns taliald I vele shat Danbo I wButhamape rene hereck nild yo, I’ve eve it.\n",
      "I’r welg the dere – ca we ce pom. I ave amel weenlpe fac tho have rum wan sheg timams, douctr pat the\n",
      "yoa fac co ka causiav eong ged I’r cno?ondevacisgr, det – We te.\n",
      "und Trot ise, I spat.ndme bika tind therd me lhavee.\n",
      "Bore dto whoud.\n",
      "H tou go bit tidpeseiske dit ast beera.\n",
      "I teral I ennd so wo –’ras. lesher gedy tely se, don Seve and aned thanouma thetce.\n",
      "eo if peounk Ih pore I’ve ldott is popleth. I’t avecito y wechears eadlich to ho uckager ugh sis os rhe anouy in’t I’r dens anc, sas \n",
      "nort dycas’te goy I at that ibed thoe wur, have cand har tisiniosey lopre ckney thes’breg anecn cnn an dessrnd theo roll beeghr soalshere cue’nt p ang.\n",
      "BWecel knrot fo sasticant riwk Ousent, \n",
      "Amt thinet In,’srde fre whes kige nhitt nite he ther I dece, herletsas Man fit sing irok.\n",
      "\"Souse of casa tialing dinten n beth ,er I duauc wthistied Isos al iin ort raalkna dape chic hot’t piglert I’tode tho tos ane tellle of ipoung jut ave. In hape asthe, I’d’I de ineg al wovegna afas, wo his her hetre dect the leoult teod yo win toto anavait they te oneg ind tome whiel so I’d.\n",
      "\"So shat bed I’ pacd an wey aled key’s’t tom recigemwe.\n",
      "Shiing of nont bevedbou plithe had.\n",
      "That.\n",
      "I I rbus yut wan keng cotan, I heer she nond. ISe thiiads ruve wige ay hap he co hauy eopplaoul ter oulke thecs seedbent have an.\n",
      "Anis be, binn ang bheerpolet wed, I’t che.w\n",
      "mey ing teed tho hel sempel . Blaved sico fom gere.\n",
      "re ledy I youveont hat cef ned’hil didtdell rell dew Iey Sad that sipe gouy anot ded, know I kluat thed ane mundth.\n",
      "A gout kant a poy sot heik yNoy me.\n",
      "As paot aeve – de evet be youd Iny oust. I theos ond demakndthica, men gemn, vanth s1eorsnis pine ay he  ow a0 i knant denlk Sadt ine anding wBnotered – Al en. I thit an,e thral, obows I thith rut’s shenbt aond keoud. Iowe mpwhe tolthay an mede’s mentel nhat save. S. gout he eopere.\n",
      "I cousre.\n",
      "cond I have reve tmriclrcunt. I sot the reld I’ oist waly bopleg pik an tuvopon the weadme.sBeve dove \"Sra tou. Id than, – Helt we gap, sam rult I wo beig in now an I.\n",
      "Ant. Tun dout. me, an ut bary thot waplean’l lots shes I’sougereade the they’reg didnst ored mive theam lilk ted. He heit. Thrin. geng.\n",
      "Wer bi dous eve ham to al  of eing teaw the no weath founn ols. Bdout aveget disso tryo gaderism.\n",
      "Abeent dupeansy wan that – deer peclls o dede ce ofey thasst wrel, am ca po onng.\n",
      "I rie wesing gedouth’ s, bi wind. \n",
      "eme thhas mron’w hee,’ulrars. I’s poniathtcapme.\n",
      "A wiem. Ave –o dounke geed maved I’ver kealieg caant \"at aoulg. gelligst thatM ma the ou kres nuch, Ave ret ate se pours hat wicane ahte utprrot’, be chel  in’t I hat Wer son’n his bealge ave ound. I\n",
      " Toui – me It ru’d I lensabe ho is nntth dee noth mo reke yond has wI see yo klouyt I’r’td s dow h pid gody whappy Inv  nobuc,. With mod.\n",
      "I enon bout?rop. s.\n",
      "Alet.\n",
      "SWe an.\n",
      "Abiout yNow woi lecka gat they o bed I cnat thou’ thoond s aiontht’mre tout, I’ frel paalmt toobettppand Jnos so ta wond \"ourhe yor Ses ane she roume to het raved d’s yriga.\n",
      "I she kecte te dos bnoully Nin a kyoter thing abe buoknd I keoe soelchiy’ I we kfoll dnge lech knnt aatull tote che sore ve eunt tha.\n",
      "At taout do shat thang endt ans mill c ca ine, they at’s ley und rid kerec ouse ad het’ne Inatrey’t af poplle? I dito atute in prot so deded’s sarlise, wi Pa coy Thimedo gen atioleth knst sind ylet and eatp ol ko gos hi rend lecs co toe.\n",
      "thas’t keot go ang I hat.\n",
      "I I woveie kat sed geing onet thayse sa lecn.\n",
      "OOpeouts talontt red Funat docall vere.\n",
      "o gishye sis on thac dave. se lis sorrit ind I Ich he kare you I ald hesy I’ theast fule.\n",
      "\"Sremet soud be donsit and maple.\n",
      "I ruow ant’s I am, ’avet. nhice ice at aad a nelld dictey an to faabemping des.\n",
      "I Iorsy what wevet geos poppsh sek’ve – whe lcaitheong it the thi\n",
      "int we hive ruy’t inedi. duy. I avell inegst wedrer the they and wave the wiss wedrotr athandrathandad \n",
      "mandte.\n",
      "re dicans aguy snhade wo mimndy An. I ag.\n",
      "nond te lo Miot weadtang thcthey’sf I ghat.\n",
      "I\n",
      "I’shy ata eitin. I’ve soll  ound. Adl baw hor soid drom os, –\n",
      "BToume olle simdiout be’st.\n",
      ".\n",
      "Shey nad beare.O’sst be I wing dingopt dikn, I heded han. I we.\n",
      "ant. Soup thas heverid.\n",
      "AHed.\n",
      "I’rbeve to keow mond I doat, ne aty int now to f tinid yo shaplabseicere thaaknps cowe Som\n"
     ]
    }
   ],
   "source": [
    "print(sample_chars(X_seed, rnn.init_h(), 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-05 *\n",
      "       -4.3225)\n",
      "tensor(1.00000e-03 *\n",
      "       -4.0199)\n",
      "tensor(1.00000e-04 *\n",
      "       7.4434)\n",
      "tensor(1.00000e-03 *\n",
      "       -4.0199)\n",
      "tensor(1.00000e-11 *\n",
      "       1.1821)\n",
      "tensor(1.00000e-10 *\n",
      "       1.4043)\n"
     ]
    }
   ],
   "source": [
    "for p in rnn.parameters():\n",
    "    print(p.grad.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w = torch.tensor([[2,3],[4,5]], dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[7,3],[3,6],[6,9]], dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wx = torch.mm(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = wx.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.,  16.],\n",
       "        [ 18.,  18.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  9.],\n",
       "        [ 5.,  9.],\n",
       "        [ 5.,  9.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 26.,  36.],\n",
       "        [ 30.,  39.],\n",
       "        [ 48.,  63.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2 = torch.mm(wx,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Why.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs[t] = np.zeros((vocab_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs[t][inputs[t]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Wxh, xs[t]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys[t] = np.dot(Why, hs[t]) + by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[t].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.3183783825123"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1.0/3000)*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hprev = hs[t-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ikama enaliia Janden Celen Badkia Darine Jumah Jagkida Asmyanat Caima Ediouy Inriserya Janieghebe Jame Giyne Jozabkatgh Alance Jomvi Jorielya Eyah Aillana Kateliuh Babye Makjan Calisse Tynnibele Jaigz'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(ix_to_char[ix] for ix in sample(hprev, 1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_ix = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = hprev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Wxh, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 53)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.25729783880305"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(vocab_size)*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hprev = np.zeros((hidden_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma Olivia Sophia Isabel'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[p:p+seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mma Olivia Sophia Isabell'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[p+1:p+seq_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " PfKTwPfmTLjbeVZCLEYRbGltieocPNNtsnTWVZNjehJnZLQRvvrMFQNpxCxHQncWpvjBYZYqasQfIdjPqtrPTEYDcCOxngJamYZCVpjyxVEIQRCbvbqpYtPM ZNVXcNiIDRixwFDVnkWUscUzu juwCkYuBqHeUVcyiWELKcoIr nr TKNCDpxLGyyKENSvJnmvweRQQ \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0398582039920804"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t][targets[t]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.22462687e-02],\n",
       "       [3.70884873e-04],\n",
       "       [1.47718282e-01],\n",
       "       [4.91778089e-04],\n",
       "       [1.43579039e-02],\n",
       "       [1.94613594e-03],\n",
       "       [3.98582040e-02],\n",
       "       [1.83239784e-03],\n",
       "       [1.40377295e-03],\n",
       "       [2.25524356e-03],\n",
       "       [1.86338027e-02],\n",
       "       [1.82163330e-03],\n",
       "       [1.36034842e-03],\n",
       "       [5.34832575e-03],\n",
       "       [5.31882994e-03],\n",
       "       [2.56723943e-02],\n",
       "       [2.08954666e-01],\n",
       "       [1.67814105e-04],\n",
       "       [5.43953948e-02],\n",
       "       [1.23416029e-02],\n",
       "       [2.71522941e-02],\n",
       "       [7.07506732e-04],\n",
       "       [3.90479874e-03],\n",
       "       [3.07619937e-03],\n",
       "       [6.03594339e-04],\n",
       "       [2.60599790e-02],\n",
       "       [3.70108202e-05],\n",
       "       [2.20467155e-03],\n",
       "       [8.62683131e-03],\n",
       "       [6.98119822e-04],\n",
       "       [5.34681858e-03],\n",
       "       [2.66708994e-03],\n",
       "       [1.11460252e-02],\n",
       "       [1.62131224e-03],\n",
       "       [4.59798728e-03],\n",
       "       [2.41688939e-03],\n",
       "       [7.15391514e-04],\n",
       "       [1.00846412e-03],\n",
       "       [7.64864017e-04],\n",
       "       [1.19025338e-02],\n",
       "       [4.88808782e-02],\n",
       "       [6.65579358e-02],\n",
       "       [1.07478080e-03],\n",
       "       [6.73335329e-04],\n",
       "       [2.65998096e-03],\n",
       "       [3.20836611e-02],\n",
       "       [2.08930948e-02],\n",
       "       [3.64345458e-04],\n",
       "       [1.19719124e-01],\n",
       "       [6.08244844e-04],\n",
       "       [5.51525083e-04],\n",
       "       [4.71561737e-03],\n",
       "       [2.94634093e-02]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps[t].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11848706, -0.05649749,  0.02691294,  0.19014995,  0.07061441,\n",
       "       -1.16252144,  0.12697324, -0.08889963, -0.06267175, -0.40105323,\n",
       "       -0.18747102, -0.18140617,  0.07432117,  0.34334437,  0.06726984,\n",
       "        0.20165935,  0.32747234,  0.04495337,  0.1142929 , -0.05521551,\n",
       "        0.00885307,  0.33516984,  0.13067932, -0.10904185,  0.19814699,\n",
       "        0.0913149 , -0.06236525, -0.01253117,  0.41185781, -0.3912567 ,\n",
       "       -0.1494085 ,  0.36435163, -0.44824475, -0.09278199,  0.34402929,\n",
       "       -0.15928979, -0.26921809,  0.05811367, -0.13787257,  0.20121986,\n",
       "        0.01388096,  0.07998907,  0.05387593,  0.0689815 ,  0.26390591,\n",
       "        0.0065182 , -0.09641561,  0.20798219,  0.07988806,  0.20825779,\n",
       "       -0.26811509, -0.03696904, -0.16780612, -0.38446364,  0.28463213,\n",
       "        0.14157797, -0.19357392, -0.16630077,  0.11378831,  0.25725707,\n",
       "        0.17818427,  0.008875  , -0.06520907,  0.12799508,  0.22632408,\n",
       "       -0.17792747,  0.19119546, -0.04671774, -0.45145593,  0.05777601,\n",
       "       -0.09830163, -0.31653179,  0.15220932,  0.20794963, -0.03008459,\n",
       "       -0.26965372,  0.1204601 ,  0.34252047,  0.09401755,  0.52394405,\n",
       "       -1.28136092, -2.30766055,  0.0621971 ,  0.65735221, -0.06726325,\n",
       "       -0.10967955,  0.37269327, -0.12546259,  0.03567711, -0.12430217,\n",
       "       -0.11907313, -0.0464631 ,  0.29016598,  0.23329175,  0.22403798,\n",
       "        0.04957504, -0.03167637, -0.15892867, -0.21109618, -0.03625879])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh[:,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
