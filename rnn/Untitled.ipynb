{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as ply\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([2.0])\n",
    "\n",
    "class Linear_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lm1 = nn.Linear(1,1,bias=False)\n",
    "        self.lm2 = nn.Linear(1,1,bias=True)\n",
    "    def forward(self, X):\n",
    "        out = self.lm1(X) + self.lm2(X)\n",
    "        \n",
    "        return out\n",
    "\n",
    "mdl = Linear_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.2707]]), Parameter containing:\n",
      "tensor([[-0.7197]]), Parameter containing:\n",
      "tensor([ 0.2062])]\n"
     ]
    }
   ],
   "source": [
    "print(list(mdl.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7746])\n"
     ]
    }
   ],
   "source": [
    "print(mdl(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is 414.9129943847656 at iter 0, weight: 0.8962091207504272\n",
      "loss is 390.3402404785156 at iter 10, weight: 0.6797359585762024\n",
      "loss is 348.17059326171875 at iter 20, weight: 0.4752900302410126\n",
      "loss is 310.55670166015625 at iter 30, weight: 0.2822031080722809\n",
      "loss is 277.0063171386719 at iter 40, weight: 0.09984404593706131\n",
      "loss is 256.5 at iter 50, weight: 0.0\n",
      "loss is 256.5 at iter 60, weight: 0.0\n",
      "loss is 256.5 at iter 70, weight: 0.0\n",
      "loss is 256.5 at iter 80, weight: 0.0\n",
      "loss is 256.5 at iter 90, weight: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.]]), Parameter containing:\n",
       " tensor([ 0.])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.arange(10).view(-1,1)\n",
    "y = -3 * x\n",
    "\n",
    "class NN_Linear_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lm = nn.Linear(1,1)\n",
    "    def forward(self, X):\n",
    "        out = self.lm(X)\n",
    "        \n",
    "        return out\n",
    "\n",
    "mdl = NN_Linear_Model()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(mdl.parameters(), lr=0.0001)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = mdl(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    for p in mdl.parameters():\n",
    "        p.data.clamp_(0)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'loss is {loss} at iter {i}, weight: {list(mdl.parameters())[0].item()}')\n",
    "\n",
    "list(mdl.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.00000e-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.071552"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01 * -7.1552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mobject\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[1;36m0x0000000001C9AD60\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Implements stochastic gradient descent (optionally with momentum).\n",
       "\n",
       "Nesterov momentum is based on the formula from\n",
       "`On the importance of initialization and momentum in deep learning`__.\n",
       "\n",
       "Args:\n",
       "    params (iterable): iterable of parameters to optimize or dicts defining\n",
       "        parameter groups\n",
       "    lr (float): learning rate\n",
       "    momentum (float, optional): momentum factor (default: 0)\n",
       "    weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
       "    dampening (float, optional): dampening for momentum (default: 0)\n",
       "    nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
       "\n",
       "Example:\n",
       "    >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
       "    >>> optimizer.zero_grad()\n",
       "    >>> loss_fn(model(input), target).backward()\n",
       "    >>> optimizer.step()\n",
       "\n",
       "__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
       "\n",
       ".. note::\n",
       "    The implementation of SGD with Momentum/Nesterov subtly differs from\n",
       "    Sutskever et. al. and implementations in some other frameworks.\n",
       "\n",
       "    Considering the specific case of Momentum, the update can be written as\n",
       "\n",
       "    .. math::\n",
       "              v = \\rho * v + g \\\\\n",
       "              p = p - lr * v\n",
       "\n",
       "    where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
       "    velocity, and momentum respectively.\n",
       "\n",
       "    This is in contrast to Sutskever et. al. and\n",
       "    other frameworks which employ an update of the form\n",
       "\n",
       "    .. math::\n",
       "         v = \\rho * v + lr * g \\\\\n",
       "         p = p - v\n",
       "\n",
       "    The Nesterov version is analogously modified.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\program files (x86)\\python\\anaconda\\envs\\py36\\lib\\site-packages\\torch\\optim\\sgd.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.optim.SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def init(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super(DecoderRNN, self).init()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings),1)\n",
    "        hiddens,_ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = embed_size\n",
    "hidden_size = \n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".forward(batch_size, caption_length+1, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = caption_length+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: (seq_len, batch, num_directions * hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
       "sequence.\n",
       "\n",
       "\n",
       "For each element in the input sequence, each layer computes the following\n",
       "function:\n",
       "\n",
       ".. math::\n",
       "\n",
       "        \\begin{array}{ll}\n",
       "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
       "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
       "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
       "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
       "        c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
       "        h_t = o_t \\tanh(c_t)\n",
       "        \\end{array}\n",
       "\n",
       "where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
       "state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}`\n",
       "is the hidden state of the previous layer at time `t-1` or the initial hidden\n",
       "state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n",
       ":math:`o_t` are the input, forget, cell, and output gates, respectively.\n",
       ":math:`\\sigma` is the sigmoid function.\n",
       "\n",
       "Args:\n",
       "    input_size: The number of expected features in the input `x`\n",
       "    hidden_size: The number of features in the hidden state `h`\n",
       "    num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
       "        would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
       "        with the second LSTM taking in outputs of the first LSTM and\n",
       "        computing the final results. Default: 1\n",
       "    bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
       "        Default: ``True``\n",
       "    batch_first: If ``True``, then the input and output tensors are provided\n",
       "        as (batch, seq, feature)\n",
       "    dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
       "        LSTM layer except the last layer, with dropout probability equal to\n",
       "        :attr:`dropout`. Default: 0\n",
       "    bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
       "\n",
       "Inputs: input, (h_0, c_0)\n",
       "    - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
       "      of the input sequence.\n",
       "      The input can also be a packed variable length sequence.\n",
       "      See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
       "      :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
       "    - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
       "      containing the initial hidden state for each element in the batch.\n",
       "    - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
       "      containing the initial cell state for each element in the batch.\n",
       "\n",
       "      If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.\n",
       "\n",
       "\n",
       "Outputs: output, (h_n, c_n)\n",
       "    - **output** of shape `(seq_len, batch, hidden_size * num_directions)`: tensor\n",
       "      containing the output features `(h_t)` from the last layer of the LSTM,\n",
       "      for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
       "      given as the input, the output will also be a packed sequence.\n",
       "    - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
       "      containing the hidden state for `t = seq_len`\n",
       "    - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
       "      containing the cell state for `t = seq_len`\n",
       "\n",
       "Attributes:\n",
       "    weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)`\n",
       "    weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
       "        `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)`\n",
       "    bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
       "    bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
       "        `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> rnn = nn.LSTM(10, 20, 2)\n",
       "    >>> input = torch.randn(5, 3, 10)\n",
       "    >>> h0 = torch.randn(2, 3, 20)\n",
       "    >>> c0 = torch.randn(2, 3, 20)\n",
       "    >>> output, hn = rnn(input, (h0, c0))\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\program files (x86)\\python\\anaconda\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1.1]).float().long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINIModel(nn.Module):\n",
    "    def __init__(self,h0=None):\n",
    "        super(MINIModel, self).__init__()\n",
    "        self.h = nn.Linear(3,4)\n",
    "        if not h0 is None:\n",
    "            self.h.weight.data = h0\n",
    "    def forward(self,x):\n",
    "        return self.h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
