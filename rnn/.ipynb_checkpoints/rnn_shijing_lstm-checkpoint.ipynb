{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/shijing.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=''.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41804"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'有巢，維鳩盈之，之子于歸，百兩成之。\\n\\n《召南・采蘩》\\n于以采蘩，于沼于沚，于以用之，公侯之事。\\n于以采蘩，于澗之中，于以用之，公侯之宮。\\n被之僮僮，夙夜在公，被之祁祁，薄言還歸。\\n\\n《召南・草蟲》\\n喓喓草蟲，趯趯阜螽。未見君子，憂心忡忡；亦既見止，亦既覯止，我心則降。\\n陟彼南山，言采其蕨。未見君子，憂心惙惙；亦既見止，亦既覯止，我心則說。\\n陟彼南山，言采其薇。未見君子，我心傷悲；亦既見止，亦既覯'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 41804 characters, 2848 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 128 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(data), len(chars)))\n",
    "\n",
    "char_id = np.array([chars.index(c) for c in data])\n",
    "\n",
    "X_train[np.arange(len(X_train)), char_id] = 1\n",
    "\n",
    "y_train = np.roll(char_id,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41804, 2848)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41804,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41800"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)//seq_length * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X=X_train, y=y_train, batch_size=seq_length):\n",
    "    #X_ids = list(range(len(X)))\n",
    "    #random.shuffle(X_ids)    \n",
    "    #X = X[X_ids]\n",
    "    #y = y[X_ids]\n",
    "    #truncate_id = len(X_train)//seq_length * seq_length\n",
    "    X = torch.from_numpy(X).float()\n",
    "    y = torch.from_numpy(y).long()\n",
    "    for i in range(0, len(y)-seq_length):   \n",
    "        yield([X[i:i+seq_length], y[i:i+seq_length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_chars(X_seed, h_prev, length=20):\n",
    "    #for p in rnn.parameters():\n",
    "    #    p.requires_grad = False\n",
    "    X_next = X_seed\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):        \n",
    "            y_score, h_prev = rnn(X_batch.view(seq_length,1,-1), h_prev)\n",
    "            y_prob = nn.Softmax(0)(y_score.view(-1)).detach().numpy()\n",
    "            y_pred = np.random.choice(chars,1, p=y_prob).item()\n",
    "            results.append(y_pred)\n",
    "            X_next_char = torch.zeros_like(X_seed[0])\n",
    "            X_next_char[chars.index(y_pred)] = 1\n",
    "            X_next = torch.cat((X_next, X_next_char.reshape(1,-1)))[1:]\n",
    "            #print(f'{i} th char:{y_pred}')|\n",
    "    #for p in rnn.parameters():\n",
    "    #    p.requires_grad = True\n",
    "    return ''.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        _, hidden = self.lstm(X, hidden)\n",
    "        output = self.out(hidden[0])\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size)\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn_LSTM(vocab_size, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in get_batch(X_train, y_train, seq_length):\n",
    "#    print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch(X_train, y_train, seq_length).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "        batch_loss = torch.tensor(0.0)\n",
    "        X_batch, y_batch = batch\n",
    "        h_prev = rnn.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2848])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn(X_batch[1].view(1,1,-1), h_prev)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "        y_score, h_prev = rnn(X_batch[i].view(1,1,-1), h_prev)\n",
    "        loss = loss_fn(y_score.view(1,-1), y_batch[i].view(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.0136)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'言笱鳧縶歲潰蘖疑毗罿姑局稱設罩株妻飢閑甲配駪固齒獸寒閑誘泱工瑣瘳嘻陽橐釃昔同株蟊剛荏綦慘煽南忌紕蓋徐孟榦兕悲隤沚駰牀光燬桋朕弋瞻嘅庤試匹夫貶奰筵隘驂悄惽岸樕縭拳駓辭屨侜慅蜉微用伉鬱醜檜鍛泲暴旒舉逃匱發敝握迋虧膏伴孺具隮旦大菡爰汙駵浪愾欣娛漂鳩瀏彭虔畏條商墠厲攸尸樅韠蹙敦溱疐釋索伊鳴愒扤洛售汜自蘇緡赬蓆雀鼈粺負愉寒惕鬵覺瞻雷褒鄭穆厹澣壬寬盡俟萏拊候烋走溢繩芹芸褎羣襜糦巾》鱮卷趙適曲替貳縞束枕闞寡浼弨蘭挃瀼磬獸檉僊惴贈稽膺并黼詹霾陽敖儀鬩愧熲尸莎娛卑之弘戢騂芬力聲造藩肇淪草繆瘼扡雊怨鋪屢寇襄熇瓞屎波坐答貳鷊雞恭逆黑畀盜櫜淺駱電撻拚靈誓凝瑕歌喓弭它瀌坰讎垤拊還染孔煇翼鳶考源漆宜厹槃煇偕鰥磋赳訪相慎浦青兔抽瞍蔦代掘肄騰傷淺柯麀潀狄壺湄淺肯日牖嘉奕庶亦副彭詛耘藩尨忝牡苓河戰體鬼巘貳委周矣夜蒿詒幅僭律又甲達蓬鞏頃驟玖汔琚塵苾選翔琴訌寐盼猱淵晞戰均廓奮磨鰷胾盾之疚枌取璊衛鸒霢葑心埸焞眾姝日伏粟簧舍括臻罄手追蕡俎麃韡邪固孫齊瑳怭合沓葽壽席渰旬缶鈴習詭誕爵治窒晨夙咮極苗卬淪膏暱斁酲乘駵宴襮韘媚駒輯雅龜忽幪桓燠榛襛闐濱嘆僮黃牙棧述聖沮虔畽補莎廩遯流雙澗恃北秦蜾蘞熙駮羅房篪會合彝題佼貉隅嶽階瓊輻球垂獻惴懌吾'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_chars(X_batch, h_prev, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_batch, y_batch):\n",
    "    h_prev = rnn.initHidden()\n",
    "    optimizer.zero_grad()\n",
    "    batch_loss = torch.tensor(0)\n",
    "    \n",
    "    for i in range(len(X_batch)):\n",
    "        y_score, h_prev = rnn(X_batch[i].view(1,1,-1), h_prev)\n",
    "        loss = loss_fn(y_score.view(1,-1), y_batch[i].view(1))\n",
    "        batch_loss += loss\n",
    "    batch_loss.backward()\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate    \n",
    "    optimizer.step()\n",
    "\n",
    "    return y_score, batch_loss/len(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f'logs/lstm1_{time.strftime(\"%Y%m%d-%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "print_every = 100\n",
    "for epoch in range(100):    \n",
    "    for batch in get_batch(X_train, y_train, seq_length):\n",
    "        _, batch_loss = train(X_batch, y_batch)\n",
    "        for p in rnn.parameters():\n",
    "            p.grad = torch.clamp(p.grad, -5,5)\n",
    "        optimizer.step()\n",
    "        all_losses.append(batch_loss.item())\n",
    "        if len(all_losses)%print_every==0:\n",
    "            print(f'----\\nRunning Avg Loss:{np.mean(all_losses[-print_every:])} at iter: {len(all_losses)}\\n----')\n",
    "            writer.add_scalar('loss', np.mean(all_losses[-100:]), len(all_losses))\n",
    "            print(sample_chars(X_batch, h_prev, print_every))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "for epoch in range(100):    \n",
    "    for batch in get_batch(X_train, y_train, seq_length):\n",
    "        X_batch, y_batch = batch\n",
    "        h_prev = rnn.initHidden()\n",
    "        y_score, h_prev = rnn(X_batch.view(seq_length,1,-1), h_prev)\n",
    "        loss = loss_fn(y_score.view(1,-1), y_batch[-1].view(1))\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        for p in rnn.parameters():\n",
    "            p.grad = torch.clamp(p.grad, -5,5)\n",
    "        optimizer.step()\n",
    "        all_losses.append(loss.item())\n",
    "        if len(all_losses)%100==0:\n",
    "            print(f'----\\nRunning Avg Loss:{np.mean(all_losses[-100:])} at iter: {len(all_losses)}\\n----')\n",
    "            writer.add_scalar('loss', np.mean(all_losses[-100:]), len(all_losses))\n",
    "            print(sample_chars(X_batch, h_prev, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1600 * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1672"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41804//25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41800"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1672*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
