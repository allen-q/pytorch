{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as ply\n",
    "import os\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/skainkaryam/basic-data-visualization-using-pytorch-dataset\n",
    "class TGSSaltDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, root_path, file_list):\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_folder = os.path.join(self.root_path, \"images\")\n",
    "        image_path = os.path.join(image_folder, file_id + \".png\")\n",
    "        \n",
    "        mask_folder = os.path.join(self.root_path, \"masks\")\n",
    "        mask_path = os.path.join(mask_folder, file_id + \".png\")\n",
    "        \n",
    "        image = np.array(imageio.imread(image_path), dtype=np.uint8)\n",
    "        mask = np.array(imageio.imread(mask_path), dtype=np.uint8)\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_depth = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./data/train/\"\n",
    "train_file_list = list(df_depth['id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TGSSaltDataset(train_path, train_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/train/'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(imageio.imread('./data/train/images/000e218f21.png'), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131, 131, 131], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120.991"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_img[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_to_df(img_path, normalize=False, mean_img=None):\n",
    "    images = []\n",
    "    for filename in glob.glob(f'{img_path}/*.png'): #assuming gif\n",
    "        img_id = filename.split('\\\\')[-1].split('.')[0]\n",
    "        img = np.array(imageio.imread(filename), dtype=np.float)\n",
    "        if normalize:\n",
    "            img -= mean_img[:,:,None]\n",
    "        images.append([img_id, img])\n",
    "    return pd.DataFrame(images, columns=['img_id', 'img']).set_index('img_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_to_np(img_path):\n",
    "    images = []\n",
    "    for filename in glob.glob(f'{img_path}/*.png'): #assuming gif\n",
    "        img_id = filename.split('\\\\')[-1].split('.')[0]\n",
    "        images.append(np.array(imageio.imread(filename), dtype=np.uint8))\n",
    "    return np.r_[images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = load_img_to_np('./data/train/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np_train[:,:,:,0].reshape(np_train.shape[0], -1).mean(0).reshape(101,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = load_img_to_df('./data/train/images', normalize=True, mean_img=mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['mask'] = load_img('./data/train/masks/')['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = load_img_to_df('./data/test/images', normalize=True, mean_img=mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_test = np.r_[df_test.img.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6330799240815169"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_test[:,:,:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train = np.r_[df_train.img.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 101, 101, 3)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies a 3D convolution over an input signal composed of several input\n",
       "planes.\n",
       "\n",
       "In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`\n",
       "and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    \\begin{equation*}\n",
       "    \\text{out}(N_i, C_{out_j}) = \\text{bias}(C_{out_j}) +\n",
       "                            \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{out_j}, k) \\star \\text{input}(N_i, k)\n",
       "    \\end{equation*},\n",
       "\n",
       "where :math:`\\star` is the valid 3D `cross-correlation`_ operator\n",
       "\n",
       "* :attr:`stride` controls the stride for the cross-correlation.\n",
       "\n",
       "* :attr:`padding` controls the amount of implicit zero-paddings on both\n",
       "  sides for :attr:`padding` number of points for each dimension.\n",
       "\n",
       "* :attr:`dilation` controls the spacing between the kernel points; also known as the Ã  trous algorithm.\n",
       "  It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.\n",
       "\n",
       "* :attr:`groups` controls the connections between inputs and outputs.\n",
       "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
       "  :attr:`groups`. For example,\n",
       "\n",
       "    * At groups=1, all inputs are convolved to all outputs.\n",
       "    * At groups=2, the operation becomes equivalent to having two conv\n",
       "      layers side by side, each seeing half the input channels,\n",
       "      and producing half the output channels, and both subsequently\n",
       "      concatenated.\n",
       "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
       "      its own set of filters (of size\n",
       "      :math:`\\left\\lfloor\\frac{\\text{out_channels}}{\\text{in_channels}}\\right\\rfloor`).\n",
       "\n",
       "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
       "\n",
       "    - a single ``int`` -- in which case the same value is used for the depth, height and width dimension\n",
       "    - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,\n",
       "      the second `int` for the height dimension and the third `int` for the width dimension\n",
       "\n",
       ".. note::\n",
       "\n",
       "     Depending of the size of your kernel, several (of the last)\n",
       "     columns of the input might be lost, because it is a valid `cross-correlation`_,\n",
       "     and not a full `cross-correlation`_.\n",
       "     It is up to the user to add proper padding.\n",
       "\n",
       ".. note::\n",
       "\n",
       "     The configuration when `groups == in_channels` and `out_channels == K * in_channels`\n",
       "     where `K` is a positive integer is termed in literature as depthwise convolution.\n",
       "\n",
       "     In other words, for an input of size :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`, if you want a\n",
       "     depthwise convolution with a depthwise multiplier `K`,\n",
       "     then you use the constructor arguments\n",
       "     :math:`(\\text{in_channels}=C_{in}, \\text{out_channels}=C_{in} * K, ..., \\text{groups}=C_{in})`\n",
       "\n",
       "Args:\n",
       "    in_channels (int): Number of channels in the input image\n",
       "    out_channels (int): Number of channels produced by the convolution\n",
       "    kernel_size (int or tuple): Size of the convolving kernel\n",
       "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
       "    padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0\n",
       "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
       "    groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\n",
       "    bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`\n",
       "    - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where\n",
       "\n",
       "      .. math::\n",
       "          D_{out} = \\left\\lfloor\\frac{D_{in} + 2 * \\text{padding}[0] - \\text{dilation}[0]\n",
       "                * (\\text{kernel_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
       "\n",
       "          H_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding}[1] - \\text{dilation}[1]\n",
       "                * (\\text{kernel_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
       "\n",
       "          W_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding}[2] - \\text{dilation}[2]\n",
       "                * (\\text{kernel_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n",
       "\n",
       "Attributes:\n",
       "    weight (Tensor): the learnable weights of the module of shape\n",
       "                     (out_channels, in_channels, kernel_size[0], kernel_size[1], kernel_size[2])\n",
       "    bias (Tensor):   the learnable bias of the module of shape (out_channels)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With square kernels and equal stride\n",
       "    >>> m = nn.Conv3d(16, 33, 3, stride=2)\n",
       "    >>> # non-square kernels and unequal stride and with padding\n",
       "    >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
       "    >>> input = torch.randn(20, 16, 10, 50, 100)\n",
       "    >>> output = m(input)\n",
       "\n",
       ".. _cross-correlation:\n",
       "    https://en.wikipedia.org/wiki/Cross-correlation\n",
       "\n",
       ".. _link:\n",
       "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\allen\\anaconda3\\envs\\ds\\lib\\site-packages\\torch\\nn\\modules\\conv.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Conv3d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "    >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
    "    >>> input = torch.randn(20, 16, 16)\n",
    "    >>> output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 16])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 33, 7])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
